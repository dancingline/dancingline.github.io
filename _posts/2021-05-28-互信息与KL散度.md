---
layout: post
title: "互信息与KL散度"
date: 2021-05-28
excerpt: 
tags: [信息论, 聚类]
feature: 
comments: false
---



前阵子看论文遇见了KL散度，碰巧文章的metrics用的是互信息（mutual information，MI），总觉得这两个东西长得很像，于是尝试从KL散度的方向来解读一下互信息。

##### KL散度

KL散度用来量化两种概率分配之间的差异，形式上像一个相对熵，可以理解为期望的信息损失，KL散度的公式


$$
\mathrm{D_{KL}}(P \| Q)=\sum_{i=1}^{N} p_{x_{i}} \log \frac{p_{x_{i}}}{q_{x_{i}}}
$$


KL散度表示有一组观察到的数据$x_{1}, x_{2}, ..., x_{n}$，满足概率分布P，在用另一个概率分布Q来估计P的情况下，期望造成的信息损失，改写一下就是


$$
\begin{align}
\mathrm{D_{KL}}(P \| Q) &= \sum_{i=1}^{N} p_{x_{i}} (\log {p_{x_{i}}} - \log {q_{x_{i}}}) \\
&= \sum_{i=1}^{N} p_{x_{i}} \log {p_{x_{i}}} - \sum_{i=1}^{N} p_{x_{i}} \log {q_{x_{i}}}\\
&= H(P \| Q) - H(P)
\end{align}
$$


其中H表示香农信息熵，$H(P \| Q)$表示用Q估计P的熵。

##### 互信息

互信息是用来评估预测的聚类结果$C^{\dagger}$和真实的类标签$C$之间相关程度的指标，公式如下


$$
\operatorname{MI}\left(C, C^{\dagger}\right)=\sum_{c_{i} \in C, c_{j}^{\dagger} \in C^{\dagger}} p\left(c_{i}, c_{j}^{\dagger}\right) \log _{2} \frac{p\left(c_{i}, c_{j}^{\dagger}\right)}{p\left(c_{i}\right) p\left(c_{j}^{\dagger}\right)}
$$


其中$p\left(c_{i}\right) / p\left(c_{j}^{\dagger}\right) / p\left(c_{i}, c_{j}^{\dagger}\right)$分别表示任取一个数据点属于对应类的概率。

我们发现这个式子跟KL散度的很像，从信息论和KL散度的角度来解读这个公式，对于观察到的一个关于$C$和$C^{\dagger}$的联合分布，用独立假设来估计它，所造成的信息损失越大，说明$C$和$C^{\dagger}$越不独立，也就是越相关。